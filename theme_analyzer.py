#!/usr/bin/env python3
"""
Theme Analyzer for Academic Research

Identifies themes, concepts, and patterns in academic texts.
Works with markdown files generated by the PDF processor.

Usage:
    python theme_analyzer.py --input <markdown_file_or_directory> --output <output_directory>
"""

import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple
from collections import Counter, defaultdict
import json


class ThemeAnalyzer:
    """Analyze themes and concepts in academic texts."""
    
    def __init__(self):
        # Stop words (common words to ignore) - English and Slovak
        self.stop_words = self._load_stop_words()
        
        # Results storage
        self.term_frequencies = Counter()
        self.term_contexts = defaultdict(list)  # term -> [contexts where it appears]
        self.cooccurrences = defaultdict(Counter)  # term -> {cooccurring_terms: counts}
        self.documents_processed = []
        
        # Configuration
        self.min_term_length = 3
        self.context_window = 50  # characters before/after term
        self.cooccurrence_window = 100  # characters for cooccurrence detection
        
    def _load_stop_words(self) -> Set[str]:
        """Load stop words for English and Slovak."""
        # Common English stop words
        english = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
            'should', 'could', 'may', 'might', 'must', 'can', 'this', 'that',
            'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'what', 'which', 'who', 'when', 'where', 'why', 'how'
        }
        
        # Common Slovak stop words
        slovak = {
            'a', 'aj', 'ale', 'alebo', 'ako', 'by', 'bol', 'bola', 'boli', 'bolo',
            'som', 'si', 'sme', 'ste', 'sú', 'je', 'má', 'mať', 'môže', 'môžu',
            'na', 'v', 'vo', 'z', 'zo', 'do', 'pre', 'pri', 'po', 'pod', 'nad',
            'o', 'od', 'za', 's', 'so', 'k', 'ku', 'že', 'aby', 'ak', 'keď',
            'ten', 'tá', 'to', 'tí', 'tie', 'toho', 'tej', 'tým', 'tento',
            'táto', 'toto', 'títo', 'tieto', 'bol', 'bola', 'bolo', 'boli'
        }
        
        return english | slovak
    
    def analyze_file(self, filepath: Path) -> Dict:
        """Analyze a single markdown file."""
        print(f"\nAnalyzing: {filepath.name}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Skip metadata section
        content = self._skip_metadata(content)
        
        # Extract terms
        terms = self._extract_terms(content)
        
        # Count frequencies
        for term in terms:
            self.term_frequencies[term] += 1
        
        # Extract contexts for significant terms
        significant_terms = [t for t, count in self.term_frequencies.most_common(50)]
        for term in significant_terms:
            contexts = self._extract_contexts(content, term)
            self.term_contexts[term].extend(contexts)
        
        # Find cooccurrences
        self._find_cooccurrences(content, significant_terms)
        
        # Document statistics
        doc_stats = {
            'filename': filepath.name,
            'total_words': len(content.split()),
            'unique_terms': len(set(terms)),
            'processed': datetime.now().isoformat()
        }
        
        self.documents_processed.append(doc_stats)
        
        return doc_stats
    
    def analyze_directory(self, dirpath: Path) -> List[Dict]:
        """Analyze all markdown files in a directory."""
        print(f"\nAnalyzing directory: {dirpath}")
        
        md_files = list(dirpath.glob('*.md'))
        
        if not md_files:
            print("No markdown files found.")
            return []
        
        print(f"Found {len(md_files)} markdown file(s)")
        
        for md_file in md_files:
            self.analyze_file(md_file)
        
        return self.documents_processed
    
    def _skip_metadata(self, content: str) -> str:
        """Skip the metadata section."""
        parts = content.split('## Extracted Text', 1)
        if len(parts) == 2:
            return parts[1]
        return content
    
    def _extract_terms(self, text: str) -> List[str]:
        """Extract meaningful terms from text."""
        # Convert to lowercase
        text = text.lower()
        
        # Extract words (including Slovak characters)
        words = re.findall(r'\b[a-záčďéíľňóôŕšťúýž]+\b', text, re.UNICODE)
        
        # Filter out stop words and short terms
        terms = [
            w for w in words 
            if w not in self.stop_words and len(w) >= self.min_term_length
        ]
        
        return terms
    
    def _extract_contexts(self, text: str, term: str) -> List[str]:
        """Extract context snippets where term appears."""
        contexts = []
        text_lower = text.lower()
        term_lower = term.lower()
        
        # Find all positions of the term
        pos = 0
        while True:
            pos = text_lower.find(term_lower, pos)
            if pos == -1:
                break
            
            # Extract context window
            start = max(0, pos - self.context_window)
            end = min(len(text), pos + len(term) + self.context_window)
            
            context = text[start:end].strip()
            contexts.append(context)
            
            pos += len(term)
        
        return contexts[:3]  # Limit to 3 contexts per term per document
    
    def _find_cooccurrences(self, text: str, terms: List[str]) -> None:
        """Find terms that frequently appear together."""
        text_lower = text.lower()
        
        for i, term1 in enumerate(terms):
            # Find positions of term1
            positions = [m.start() for m in re.finditer(r'\b' + re.escape(term1) + r'\b', text_lower)]
            
            for pos in positions:
                # Check window around term1
                window_start = max(0, pos - self.cooccurrence_window)
                window_end = min(len(text), pos + self.cooccurrence_window)
                window = text_lower[window_start:window_end]
                
                # Check for other terms in window
                for term2 in terms[i+1:]:
                    if term2 in window:
                        self.cooccurrences[term1][term2] += 1
                        self.cooccurrences[term2][term1] += 1
    
    def identify_themes(self) -> List[Dict]:
        """Identify major themes from term analysis."""
        themes = []
        
        # Get top terms
        top_terms = self.term_frequencies.most_common(30)
        
        for term, frequency in top_terms:
            # Calculate term importance (TF-IDF-like)
            # Here simplified as frequency * uniqueness
            doc_appearances = sum(1 for doc in self.documents_processed if term in str(doc))
            uniqueness = 1.0 / (1.0 + doc_appearances)
            importance = frequency * (1 + uniqueness)
            
            # Get related terms (cooccurrences)
            related = []
            if term in self.cooccurrences:
                related = [
                    {'term': t, 'strength': count}
                    for t, count in self.cooccurrences[term].most_common(5)
                ]
            
            # Get contexts
            contexts = self.term_contexts.get(term, [])[:3]
            
            theme = {
                'term': term,
                'frequency': frequency,
                'importance': round(importance, 2),
                'related_terms': related,
                'sample_contexts': contexts
            }
            
            themes.append(theme)
        
        # Sort by importance
        themes.sort(key=lambda x: x['importance'], reverse=True)
        
        return themes
    
    def identify_clusters(self) -> List[Dict]:
        """Identify clusters of related terms (concept groups)."""
        clusters = []
        processed = set()
        
        # Get highly connected terms
        top_terms = [term for term, _ in self.term_frequencies.most_common(50)]
        
        for seed_term in top_terms:
            if seed_term in processed:
                continue
            
            # Find strongly related terms
            if seed_term not in self.cooccurrences:
                continue
            
            cluster_terms = [seed_term]
            related = self.cooccurrences[seed_term].most_common(5)
            
            # Add related terms to cluster
            for related_term, strength in related:
                if strength >= 3:  # Threshold for inclusion
                    cluster_terms.append(related_term)
                    processed.add(related_term)
            
            if len(cluster_terms) > 1:
                cluster = {
                    'central_term': seed_term,
                    'related_terms': cluster_terms[1:],
                    'cohesion': len(cluster_terms),
                    'total_mentions': sum(self.term_frequencies[t] for t in cluster_terms)
                }
                clusters.append(cluster)
                processed.add(seed_term)
        
        # Sort by cohesion
        clusters.sort(key=lambda x: x['cohesion'], reverse=True)
        
        return clusters
    
    def generate_insights(self) -> Dict:
        """Generate research insights and opportunities."""
        themes = self.identify_themes()
        clusters = self.identify_clusters()
        
        # Identify dominant themes
        dominant = themes[:5] if themes else []
        
        # Identify emerging themes (mentioned but not extensively)
        emerging = [
            t for t in themes 
            if 3 <= t['frequency'] <= 10
        ][:5]
        
        # Identify gaps (terms mentioned once - potential unexplored areas)
        single_mentions = [
            term for term, count in self.term_frequencies.items()
            if count == 1
        ]
        
        insights = {
            'dominant_themes': dominant,
            'emerging_themes': emerging,
            'concept_clusters': clusters[:5],
            'potential_gaps': {
                'count': len(single_mentions),
                'examples': single_mentions[:10]
            },
            'corpus_statistics': {
                'total_documents': len(self.documents_processed),
                'unique_terms': len(self.term_frequencies),
                'total_terms': sum(self.term_frequencies.values())
            }
        }
        
        return insights
    
    def save_analysis(self, output_dir: Path, analysis_name: str = "analysis"):
        """Save analysis results."""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate insights
        insights = self.generate_insights()
        
        # Save as JSON
        json_path = output_dir / f"{analysis_name}_themes.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(insights, f, indent=2, ensure_ascii=False)
        
        # Save detailed term frequency
        freq_path = output_dir / f"{analysis_name}_frequencies.json"
        freq_data = {
            'top_50_terms': [
                {'term': term, 'frequency': count}
                for term, count in self.term_frequencies.most_common(50)
            ]
        }
        with open(freq_path, 'w', encoding='utf-8') as f:
            json.dump(freq_data, f, indent=2, ensure_ascii=False)
        
        # Generate markdown report
        self._generate_markdown_report(output_dir, analysis_name, insights)
        
        print(f"\n✓ Analysis saved to:")
        print(f"  - Insights: {json_path}")
        print(f"  - Frequencies: {freq_path}")
        print(f"  - Report: {output_dir / f'{analysis_name}_report.md'}")
    
    def _generate_markdown_report(self, output_dir: Path, analysis_name: str, insights: Dict):
        """Generate human-readable markdown report."""
        report_path = output_dir / f"{analysis_name}_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# Theme Analysis Report\n\n")
            f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Corpus statistics
            stats = insights['corpus_statistics']
            f.write(f"## Corpus Statistics\n\n")
            f.write(f"- **Documents Analyzed:** {stats['total_documents']}\n")
            f.write(f"- **Unique Terms:** {stats['unique_terms']}\n")
            f.write(f"- **Total Term Occurrences:** {stats['total_terms']}\n\n")
            
            # Dominant themes
            f.write(f"## Dominant Themes\n\n")
            for theme in insights['dominant_themes']:
                f.write(f"### {theme['term'].title()}\n\n")
                f.write(f"- **Frequency:** {theme['frequency']} occurrences\n")
                f.write(f"- **Importance Score:** {theme['importance']}\n")
                
                if theme['related_terms']:
                    f.write(f"- **Related Terms:**\n")
                    for rel in theme['related_terms']:
                        f.write(f"  - {rel['term']} (co-occurs {rel['strength']} times)\n")
                
                if theme['sample_contexts']:
                    f.write(f"\n**Sample Contexts:**\n")
                    for i, ctx in enumerate(theme['sample_contexts'][:2], 1):
                        f.write(f"{i}. \"...{ctx}...\"\n\n")
                
                f.write("\n---\n\n")
            
            # Concept clusters
            if insights['concept_clusters']:
                f.write(f"## Concept Clusters\n\n")
                f.write("Groups of related terms that frequently appear together:\n\n")
                
                for cluster in insights['concept_clusters']:
                    f.write(f"### Cluster: {cluster['central_term'].title()}\n\n")
                    f.write(f"- **Related Terms:** {', '.join(cluster['related_terms'])}\n")
                    f.write(f"- **Total Mentions:** {cluster['total_mentions']}\n\n")
            
            # Emerging themes
            if insights['emerging_themes']:
                f.write(f"## Emerging Themes\n\n")
                f.write("Terms with moderate frequency that may warrant deeper investigation:\n\n")
                for theme in insights['emerging_themes']:
                    f.write(f"- **{theme['term'].title()}** ({theme['frequency']} mentions)\n")
                f.write("\n")
            
            # Research gaps
            gaps = insights['potential_gaps']
            f.write(f"## Potential Research Gaps\n\n")
            f.write(f"Found **{gaps['count']}** terms mentioned only once, suggesting potential unexplored areas:\n\n")
            f.write("Sample underrepresented topics:\n")
            for term in gaps['examples']:
                f.write(f"- {term}\n")
    
    def print_summary(self):
        """Print analysis summary."""
        print(f"\n{'='*60}")
        print("Theme Analysis Summary")
        print(f"{'='*60}")
        print(f"\nDocuments processed: {len(self.documents_processed)}")
        print(f"Unique terms identified: {len(self.term_frequencies)}")
        print(f"Total term occurrences: {sum(self.term_frequencies.values())}")
        
        print(f"\nTop 10 most frequent terms:")
        for term, count in self.term_frequencies.most_common(10):
            print(f"  {count:4d} × {term}")
        
        print(f"\n{'='*60}\n")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Analyze themes and concepts in academic texts',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python theme_analyzer.py --input extracted.md --output ./analysis
  python theme_analyzer.py --input ./output --output ./analysis
        """
    )
    
    parser.add_argument(
        '--input', '-i',
        required=True,
        help='Markdown file or directory to analyze'
    )
    
    parser.add_argument(
        '--output', '-o',
        required=True,
        help='Directory for analysis results'
    )
    
    args = parser.parse_args()
    
    # Validate input
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"Error: Input not found: {input_path}")
        exit(1)
    
    # Create analyzer
    analyzer = ThemeAnalyzer()
    
    # Process input
    if input_path.is_file():
        analyzer.analyze_file(input_path)
    else:
        analyzer.analyze_directory(input_path)
    
    # Generate and save analysis
    output_dir = Path(args.output)
    analyzer.save_analysis(output_dir)
    
    # Print summary
    analyzer.print_summary()
    
    exit(0)


if __name__ == '__main__':
    main()